# -*- coding: utf-8 -*-
"""myntrarecommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_UetaxqTUAEj71VGadbxUtpXaqGM7sp_
"""

!pip install rake_nltk

import pandas as pd
import numpy as np

from rake_nltk import Rake
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer #tokenizes a collection of words extracted from a text doc
from ast import literal_eval #This evaluates whether an expresion is a Python datatype or not

data1 = pd.read_csv('modified_data.csv')
data1.head()

# prompt: put these 5 selected random products in liked_products dataframe

data = data1[['Product_id','BrandName','Category','Individual_category','Preprocessed_Description']].head(3000)

data.head(5)

data['BrandName'] = data['BrandName'].str.lower()
data['Category'] = data['Category'].str.lower()
data['Individual_category'] = data['Individual_category'].str.lower()
data['Preprocessed_Description'] = data['Preprocessed_Description'].str.lower()

"""# New section"""

data['keywords'] = ''

!pip install nltk
import nltk
nltk.download('punkt')
nltk.download('stopwords')
keywords_list = []
r = Rake()

for index, row in data.iterrows():
    Preprocessed_Description = row['Preprocessed_Description']

    # Extract keywords from text
    r.extract_keywords_from_text(Preprocessed_Description)

    # Get the dictionary with keywords and their scores
    keywords_dict_scores = r.get_word_degrees()

    data.at[index, 'keywords'] = list(keywords_dict_scores.keys())

data.set_index('Product_id', inplace = True)
data.head()

data['bow'] = ''
columns_to_include = ['BrandName', 'Category', 'Individual_category', 'Preprocessed_Description']

for index, row in data.iterrows():
    words = ''
    for col in columns_to_include:
        words += ' '.join(row[col]) + ' '
    data.at[index, 'bow'] = words.strip()

data.head()

print(data['bow'].head())
print(data['bow'].isnull().sum())  # Check for null values
print(data['bow'].apply(lambda x: len(x)).value_counts())

# prompt: check if bow column only cpntaons stop words

from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

def contains_only_stopwords(text):
  words = text.split()
  return all(word in stop_words for word in words)

# Apply the function to the 'bow' column
data['contains_only_stopwords'] = data['bow'].apply(contains_only_stopwords)

# Count how many rows contain only stopwords
count_only_stopwords = data['contains_only_stopwords'].sum()
print(f"Number of rows with only stopwords: {count_only_stopwords}")

from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords

stop_words = stopwords.words('english')  # Get stop words as a list

# Customize CountVectorizer to ignore stop words and include words of length 1
count = CountVectorizer(stop_words=stop_words, token_pattern=r"(?u)\b\w+\b")  # Allow single-character words

# Check if 'bow' column contains any non-stopwords before fitting
# Iterate over each document (row) in the 'bow' column
non_stopword_docs = [text for text in data['bow'] if any(word not in stop_words for word in text.split())]

if non_stopword_docs:
    count_matrix = count.fit_transform(non_stopword_docs)
    # Fit and transform only on documents with non-stopwords
    print("Count matrix created successfully.")
else:
    print("All documents in 'bow' column contain only stop words or single characters. Cannot create count matrix.")

indices = pd.Series(data.index)
indices[:5]

count_matrix

type(count_matrix)

c = count_matrix.todense()
c

print(count_matrix[0,:])

count.vocabulary_

#cosine similarity matrix
cosine_sim = cosine_similarity(count_matrix, count_matrix)
cosine_sim

def recommendations(Product_id, n, cosine_sim, data):
    recommended_products = []

    # Get index of the product that matches the Product_id
    idx = data.index.get_loc(Product_id)

    # Find highest cosine similarity scores and sort
    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending=False)

    # Get indexes of the 'n' most similar products (excluding itself)
    top_n_indexes = list(score_series.iloc[1:n+1].index)

    # Populating the list with details of top n matching products
    for i in top_n_indexes:
        recommended_products.append({
            'Product_id': data.index[i],
            'BrandName': data.at[data.index[i], 'BrandName'],
            'Description': data.at[data.index[i], 'Preprocessed_Description']
        })

    return recommended_products

# Example usage:
Product_id = 4335679  # Example Product_id
n = 5  # Number of recommendations

# Assuming 'cosine_sim' and 'data' are defined elsewhere
recommended_products = recommendations(Product_id, n, cosine_sim, data)

# Printing the recommended products
for product in recommended_products:
    print(f"Product_id: {product['Product_id']}")
    print(f"BrandName: {product['BrandName']}")
    print(f"Description: {product['Description']}")
    print()  # Empty line for separation

product = 10473520
n = 5

product

recommendations(Product_id=product, n=n, cosine_sim=cosine_sim, data=data)

indices[indices == product].index[0]

pd.Series(cosine_sim[indices[indices == product].index[0]])

import os

model.save('model_trained.h5')

# prompt: convert and save this to h5

import tensorflow as tf
# Assuming 'cosine_sim' is your model
tf.keras.models.save_model(cosine_sim, 'model_trained.h5')